{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic Tabular Data Generation Job\n",
        "\n",
        "This notebook generates synthetic tabular data using:\n",
        "- **dbldatagen** for structured data generation\n",
        "- **ai_query()** for GenAI Text columns\n",
        "- **Databricks volumes** for storage\n",
        "\n",
        "## Parameters\n",
        "The following parameters are passed from the app via job widgets:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Import required libraries\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr, col, lit\n",
        "from dbldatagen import DataGenerator, fakerText\n",
        "\n",
        "print(\"üì¶ Libraries imported successfully\")\n",
        "print(f\"   - Execution time: {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Get job parameters via widgets\n",
        "# Databricks automatically creates widgets from job parameters\n",
        "\n",
        "try:\n",
        "    # Create widgets with default values (these will be overridden by job parameters)\n",
        "    dbutils.widgets.text(\"table_name\", \"sample_table\", \"Table Name\")\n",
        "    dbutils.widgets.text(\"row_count\", \"1000\", \"Row Count\")\n",
        "    dbutils.widgets.text(\"columns\", \"[]\", \"Columns JSON\")\n",
        "    dbutils.widgets.text(\"company_name\", \"Sample Company\", \"Company Name\")\n",
        "    dbutils.widgets.text(\"company_sector\", \"Technology\", \"Company Sector\")\n",
        "    dbutils.widgets.text(\"timestamp\", datetime.now().strftime(\"%Y%m%d_%H%M%S\"), \"Timestamp\")\n",
        "    dbutils.widgets.text(\"endpoint_name\", \"databricks-gpt-oss-120b\", \"LLM Endpoint\")\n",
        "    dbutils.widgets.text(\"volume_path\", \"conor_smith.synthetic_data_app.synthetic_data_volume\", \"Volume Path\")\n",
        "    \n",
        "    # Get parameter values\n",
        "    table_name = dbutils.widgets.get(\"table_name\")\n",
        "    row_count = int(dbutils.widgets.get(\"row_count\"))\n",
        "    columns_json = dbutils.widgets.get(\"columns\")\n",
        "    company_name = dbutils.widgets.get(\"company_name\")\n",
        "    company_sector = dbutils.widgets.get(\"company_sector\")\n",
        "    timestamp = dbutils.widgets.get(\"timestamp\")\n",
        "    endpoint_name = dbutils.widgets.get(\"endpoint_name\")\n",
        "    volume_path = dbutils.widgets.get(\"volume_path\")\n",
        "    \n",
        "    print(\"üéØ Job Parameters Retrieved:\")\n",
        "    print(f\"   - Table name: {table_name}\")\n",
        "    print(f\"   - Row count: {row_count}\")\n",
        "    print(f\"   - Company: {company_name} ({company_sector})\")\n",
        "    print(f\"   - Timestamp: {timestamp}\")\n",
        "    print(f\"   - Endpoint: {endpoint_name}\")\n",
        "    print(f\"   - Volume: {volume_path}\")\n",
        "    print(f\"   - Columns JSON length: {len(columns_json)} characters\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error getting parameters: {e}\")\n",
        "    # Fallback to default values\n",
        "    table_name = \"sample_table\"\n",
        "    row_count = 1000\n",
        "    columns_json = \"[]\"\n",
        "    company_name = \"Sample Company\"\n",
        "    company_sector = \"Technology\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    endpoint_name = \"databricks-gpt-oss-120b\"\n",
        "    volume_path = \"conor_smith.synthetic_data_app.synthetic_data_volume\"\n",
        "    print(\"‚ö†Ô∏è  Using fallback default values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Parse and validate column configurations\n",
        "try:\n",
        "    columns = json.loads(columns_json)\n",
        "    print(f\"‚úÖ Parsed {len(columns)} column configurations\")\n",
        "    \n",
        "    # Show column details\n",
        "    for i, col in enumerate(columns):\n",
        "        col_name = col.get('name', 'unnamed')\n",
        "        col_type = col.get('data_type', 'Unknown')\n",
        "        print(f\"   {i+1}. {col_name} ({col_type})\")\n",
        "        \n",
        "        # Show additional details for specific types\n",
        "        if col_type == 'Integer':\n",
        "            min_val = col.get('min_value', 'not set')\n",
        "            max_val = col.get('max_value', 'not set')\n",
        "            print(f\"      ‚Üí Range: {min_val} to {max_val}\")\n",
        "        elif col_type == 'GenAI Text':\n",
        "            prompt = col.get('prompt', 'not set')\n",
        "            max_tokens = col.get('max_tokens', 'not set')\n",
        "            print(f\"      ‚Üí Prompt: {prompt[:50]}{'...' if len(prompt) > 50 else ''}\")\n",
        "            print(f\"      ‚Üí Max tokens: {max_tokens}\")\n",
        "        elif col_type == 'Custom Values':\n",
        "            values = col.get('custom_values', [])\n",
        "            weights = col.get('use_weights', False)\n",
        "            print(f\"      ‚Üí Values: {values[:3]}{'...' if len(values) > 3 else ''}\")\n",
        "            print(f\"      ‚Üí Weighted: {weights}\")\n",
        "    \n",
        "    # Add sample columns if none provided\n",
        "    if len(columns) == 0:\n",
        "        print(\"‚ö†Ô∏è  No columns configured, adding sample columns for testing\")\n",
        "        columns = [\n",
        "            {\"name\": \"id\", \"data_type\": \"Integer\", \"min_value\": 1, \"max_value\": 1000},\n",
        "            {\"name\": \"first_name\", \"data_type\": \"First Name\"},\n",
        "            {\"name\": \"last_name\", \"data_type\": \"Last Name\"},\n",
        "            {\"name\": \"bio\", \"data_type\": \"GenAI Text\", \"prompt\": \"Write a short professional bio for <first_name> <last_name>\", \"max_tokens\": 100}\n",
        "        ]\n",
        "        print(f\"   ‚Üí Added {len(columns)} sample columns\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error parsing columns: {e}\")\n",
        "    print(f\"   Raw columns_json: {columns_json}\")\n",
        "    # Use fallback columns\n",
        "    columns = [\n",
        "        {\"name\": \"id\", \"data_type\": \"Integer\", \"min_value\": 1, \"max_value\": 100},\n",
        "        {\"name\": \"name\", \"data_type\": \"First Name\"}\n",
        "    ]\n",
        "    print(f\"   ‚Üí Using {len(columns)} fallback columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Initialize Spark and create DataGenerator\n",
        "try:\n",
        "    # Get Spark session\n",
        "    spark = SparkSession.getActiveSession()\n",
        "    if spark is None:\n",
        "        spark = SparkSession.builder.appName(\"TabularDataGeneration\").getOrCreate()\n",
        "    \n",
        "    print(\"‚ö° Spark session initialized\")\n",
        "    print(f\"   - Spark version: {spark.version}\")\n",
        "    \n",
        "    # Set partition parameters for optimal performance\n",
        "    partitions_requested = min(8, max(1, row_count // 1000))  \n",
        "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(partitions_requested))\n",
        "    \n",
        "    print(f\"üîß Spark optimized for {row_count} rows ‚Üí {partitions_requested} partitions\")\n",
        "    \n",
        "    # Create DataGenerator\n",
        "    data_gen = DataGenerator(spark, rows=row_count, partitions=partitions_requested)\n",
        "    print(f\"üèóÔ∏è  DataGenerator created\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error initializing Spark/DataGenerator: {e}\")\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Add columns to DataGenerator\n",
        "genai_columns = []\n",
        "\n",
        "for col_config in columns:\n",
        "    col_name = col_config.get('name', 'unnamed_column')\n",
        "    col_type = col_config.get('data_type', 'Integer')\n",
        "    \n",
        "    print(f\"üìä Adding column '{col_name}' ({col_type})\")\n",
        "    \n",
        "    try:\n",
        "        if col_type == 'Integer':\n",
        "            min_val = col_config.get('min_value', 1)\n",
        "            max_val = col_config.get('max_value', 100)\n",
        "            data_gen = data_gen.withColumn(col_name, \"integer\", minValue=min_val, maxValue=max_val)\n",
        "            print(f\"   ‚úÖ Integer: {min_val} to {max_val}\")\n",
        "            \n",
        "        elif col_type == 'First Name':\n",
        "            try:\n",
        "                data_gen = data_gen.withColumn(col_name, text=fakerText(\"first_name\"))\n",
        "                print(f\"   ‚úÖ First name with faker\")\n",
        "            except Exception:\n",
        "                first_names = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \n",
        "                              \"William\", \"Elizabeth\", \"David\", \"Barbara\", \"Richard\", \"Susan\", \"Joseph\", \"Jessica\"]\n",
        "                data_gen = data_gen.withColumn(col_name, values=first_names)\n",
        "                print(f\"   ‚úÖ First name with predefined list ({len(first_names)} names)\")\n",
        "            \n",
        "        elif col_type == 'Last Name':\n",
        "            try:\n",
        "                data_gen = data_gen.withColumn(col_name, text=fakerText(\"last_name\"))\n",
        "                print(f\"   ‚úÖ Last name with faker\")\n",
        "            except Exception:\n",
        "                last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \"Miller\", \"Davis\",\n",
        "                             \"Rodriguez\", \"Martinez\", \"Hernandez\", \"Lopez\", \"Gonzalez\", \"Wilson\"]\n",
        "                data_gen = data_gen.withColumn(col_name, values=last_names)\n",
        "                print(f\"   ‚úÖ Last name with predefined list ({len(last_names)} names)\")\n",
        "            \n",
        "        elif col_type == 'GenAI Text':\n",
        "            # Add placeholder, will process with ai_query later\n",
        "            data_gen = data_gen.withColumn(col_name, \"string\", values=[\"\"])\n",
        "            genai_columns.append(col_config)\n",
        "            print(f\"   ‚úÖ GenAI placeholder (will use ai_query)\")\n",
        "            \n",
        "        elif col_type == 'Custom Values':\n",
        "            custom_values = col_config.get('custom_values', [''])\n",
        "            use_weights = col_config.get('use_weights', False)\n",
        "            custom_weights = col_config.get('custom_weights', [1])\n",
        "            \n",
        "            filtered_values = [v for v in custom_values if v.strip()]\n",
        "            if not filtered_values:\n",
        "                filtered_values = ['DefaultValue']\n",
        "            \n",
        "            if use_weights and len(custom_weights) >= len(filtered_values):\n",
        "                filtered_weights = custom_weights[:len(filtered_values)]\n",
        "                data_gen = data_gen.withColumn(col_name, values=filtered_values, weights=filtered_weights)\n",
        "                print(f\"   ‚úÖ Custom values with weights: {len(filtered_values)} values\")\n",
        "            else:\n",
        "                data_gen = data_gen.withColumn(col_name, values=filtered_values)\n",
        "                print(f\"   ‚úÖ Custom values: {len(filtered_values)} values\")\n",
        "        \n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Unknown column type '{col_type}', skipping\")\n",
        "            \n",
        "    except Exception as col_error:\n",
        "        print(f\"   ‚ùå Error adding column '{col_name}': {col_error}\")\n",
        "\n",
        "print(f\"\\nüìã Summary: {len(columns)} total columns, {len(genai_columns)} GenAI columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Generate initial DataFrame and process GenAI columns\n",
        "# Build the initial DataFrame\n",
        "print(f\"üèóÔ∏è  Building DataFrame with {row_count} rows...\")\n",
        "df = data_gen.build()\n",
        "\n",
        "print(f\"‚úÖ DataFrame created: {df.count()} rows √ó {len(df.columns)} columns\")\n",
        "print(f\"   - Columns: {df.columns}\")\n",
        "\n",
        "# Show sample data\n",
        "print(f\"\\nüìä Sample Data (first 3 rows):\")\n",
        "df.show(3, truncate=False)\n",
        "\n",
        "# Process GenAI Text columns with ai_query\n",
        "if genai_columns:\n",
        "    print(f\"\\nü§ñ Processing {len(genai_columns)} GenAI Text columns with ai_query\")\n",
        "    \n",
        "    def substitute_column_references_spark(prompt_template, columns):\n",
        "        \"\"\"Create Spark SQL expression for column substitution.\"\"\"\n",
        "        import re\n",
        "        column_refs = re.findall(r'<([^<>]+)>', prompt_template)\n",
        "        \n",
        "        if not column_refs:\n",
        "            return f\"'{prompt_template}'\"\n",
        "        \n",
        "        # Build concat expression for dynamic prompt\n",
        "        parts = []\n",
        "        current_pos = 0\n",
        "        \n",
        "        for match in re.finditer(r'<([^<>]+)>', prompt_template):\n",
        "            col_name = match.group(1)\n",
        "            start_pos = match.start()\n",
        "            end_pos = match.end()\n",
        "            \n",
        "            # Add text before column reference\n",
        "            if start_pos > current_pos:\n",
        "                literal_text = prompt_template[current_pos:start_pos]\n",
        "                if literal_text:\n",
        "                    parts.append(f\"'{literal_text}'\")\n",
        "            \n",
        "            # Add column reference\n",
        "            valid_columns = [col.get('name', 'unnamed_column') for col in columns]\n",
        "            if col_name in valid_columns:\n",
        "                parts.append(f\"coalesce(cast({col_name} as string), 'NULL')\")\n",
        "            else:\n",
        "                parts.append(f\"'<{col_name}>'\")\n",
        "            \n",
        "            current_pos = end_pos\n",
        "        \n",
        "        # Add remaining text\n",
        "        if current_pos < len(prompt_template):\n",
        "            literal_text = prompt_template[current_pos:]\n",
        "            if literal_text:\n",
        "                parts.append(f\"'{literal_text}'\")\n",
        "        \n",
        "        return f\"concat({', '.join(parts)})\" if len(parts) > 1 else parts[0]\n",
        "    \n",
        "    # Process each GenAI column\n",
        "    for col_config in genai_columns:\n",
        "        col_name = col_config.get('name', 'unnamed_column')\n",
        "        prompt_template = col_config.get('prompt', '')\n",
        "        \n",
        "        if prompt_template:\n",
        "            print(f\"\\nüéØ Processing GenAI column '{col_name}'\")\n",
        "            print(f\"   - Prompt: {prompt_template[:80]}{'...' if len(prompt_template) > 80 else ''}\")\n",
        "            \n",
        "            try:\n",
        "                # Enhanced prompt for table context\n",
        "                enhanced_prompt = f\"{prompt_template} Note: This will be text data in a table so omit all special formatting.\"\n",
        "                \n",
        "                # Create dynamic prompt with column substitution\n",
        "                prompt_expression = substitute_column_references_spark(enhanced_prompt, columns)\n",
        "                print(f\"   - Spark expression created\")\n",
        "                \n",
        "                # Execute ai_query\n",
        "                print(f\"   - Executing ai_query with endpoint: {endpoint_name}\")\n",
        "                df = df.withColumn(\n",
        "                    col_name,\n",
        "                    expr(f\"ai_query(endpoint => '{endpoint_name}', request => {prompt_expression})\")\n",
        "                )\n",
        "                \n",
        "                print(f\"   ‚úÖ ai_query completed for '{col_name}'\")\n",
        "                \n",
        "                # Show sample generated text\n",
        "                sample_rows = df.select(col_name).limit(2).collect()\n",
        "                for i, row in enumerate(sample_rows):\n",
        "                    text_sample = str(row[col_name])[:100] + ('...' if len(str(row[col_name])) > 100 else '')\n",
        "                    print(f\"   - Sample {i+1}: {text_sample}\")\n",
        "                \n",
        "            except Exception as ai_error:\n",
        "                print(f\"   ‚ùå Error processing GenAI column '{col_name}': {ai_error}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ All GenAI columns processed\")\n",
        "else:\n",
        "    print(f\"\\n‚ÑπÔ∏è  No GenAI columns to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Save to Volume and complete job\n",
        "try:\n",
        "    filename = f\"{table_name}_{timestamp}.csv\"\n",
        "    print(f\"üíæ Saving data to volume...\")\n",
        "    print(f\"   - Filename: {filename}\")\n",
        "    print(f\"   - Volume: {volume_path}\")\n",
        "    \n",
        "    # Convert to Pandas for clean CSV creation\n",
        "    print(f\"üìä Converting to Pandas...\")\n",
        "    pandas_df = df.toPandas()\n",
        "    print(f\"   ‚úÖ Pandas DataFrame: {len(pandas_df)} rows √ó {len(pandas_df.columns)} columns\")\n",
        "    \n",
        "    # Write to temporary location\n",
        "    temp_path = f\"/tmp/{filename}\"\n",
        "    pandas_df.to_csv(temp_path, index=False)\n",
        "    print(f\"   ‚úÖ Temporary file created: {temp_path}\")\n",
        "    \n",
        "    # Copy to volume\n",
        "    volume_file_path = f\"/Volumes/{volume_path}/{filename}\"\n",
        "    print(f\"üì§ Copying to volume: {volume_file_path}\")\n",
        "    \n",
        "    try:\n",
        "        dbutils.fs.cp(f\"file://{temp_path}\", volume_file_path)\n",
        "        print(f\"   ‚úÖ Successfully saved to volume!\")\n",
        "        \n",
        "        # Verify file\n",
        "        try:\n",
        "            file_info = dbutils.fs.ls(volume_file_path)\n",
        "            file_size = file_info[0].size if file_info else 0\n",
        "            print(f\"   üìã File size: {file_size:,} bytes\")\n",
        "        except:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not verify file size\")\n",
        "        \n",
        "        # Clean up temp file\n",
        "        dbutils.fs.rm(f\"file://{temp_path}\")\n",
        "        print(f\"   üßπ Cleaned up temporary file\")\n",
        "        \n",
        "    except Exception as volume_error:\n",
        "        print(f\"   ‚ùå Error copying to volume: {volume_error}\")\n",
        "        print(f\"   üìÅ File remains at: {temp_path}\")\n",
        "        volume_file_path = temp_path\n",
        "    \n",
        "    # Final summary\n",
        "    print(f\"\\nüéâ Job Completed Successfully!\")\n",
        "    print(f\"üìã Final Summary:\")\n",
        "    print(f\"   - Table: {table_name}\")\n",
        "    print(f\"   - Rows: {df.count():,}\")\n",
        "    print(f\"   - Columns: {len(df.columns)}\")\n",
        "    print(f\"   - GenAI columns: {len(genai_columns)}\")\n",
        "    print(f\"   - Company: {company_name} ({company_sector})\")\n",
        "    print(f\"   - File: {volume_file_path}\")\n",
        "    print(f\"   - Completed: {datetime.now()}\")\n",
        "    \n",
        "    # Show final sample\n",
        "    print(f\"\\nüìä Final Data Sample:\")\n",
        "    df.show(5, truncate=False)\n",
        "    \n",
        "except Exception as save_error:\n",
        "    print(f\"‚ùå Error during save: {save_error}\")\n",
        "    print(f\"   Job may have succeeded but file save failed\")\n",
        "    raise save_error\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
